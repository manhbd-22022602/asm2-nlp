{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0c_nafPNB05q",
        "outputId": "c3639871-5b79-4493-d45d-2313f1dce653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikiextractor\n",
            "  Downloading wikiextractor-3.0.6-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wikiextractor\n",
            "Successfully installed wikiextractor-3.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install wikiextractor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dumps.wikimedia.org/viwiki/20240701/viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2"
      ],
      "metadata": {
        "id": "kv7IwxTeB1WL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c96124-c3ad-4744-be85-94ff9829c99a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-16 16:01:44--  https://dumps.wikimedia.org/viwiki/20240701/viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3219583 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2’\n",
            "\n",
            "viwiki-20240701-pag 100%[===================>]   3.07M  3.91MB/s    in 0.8s    \n",
            "\n",
            "2024-07-16 16:01:45 (3.91 MB/s) - ‘viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2’ saved [3219583/3219583]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m wikiextractor.WikiExtractor '/content/viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7KT1IDoIwEL",
        "outputId": "77e9a3b8-0f75-44dd-db77-7d651a6bc41e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Preprocessing '/content/viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2' to collect template definitions: this may take some time.\n",
            "INFO: Loaded 171 templates in 1.1s\n",
            "INFO: Starting page extraction from /content/viwiki-20240701-pages-articles-multistream5.xml-p9680045p11180044.bz2.\n",
            "INFO: Using 1 extract processes.\n",
            "INFO: Finished 1-process extraction of 1931 articles in 3.6s (543.6 art/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuFHjLX3R7SJ",
        "outputId": "b6571af0-532a-405a-900b-3b8abc7b3757"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.10 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvi.ViTokenizer import tokenize\n",
        "import re, os, string\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub('<.*?>', '', text).strip()\n",
        "    text = re.sub('(\\s)+', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def normalize_text(text):\n",
        "    listpunctuation = string.punctuation.replace('_', '')\n",
        "    for i in listpunctuation:\n",
        "        text = text.replace(i, ' ')\n",
        "    return text.lower()\n",
        "\n",
        "# list stopwords\n",
        "filename = './stopwords.csv'\n",
        "data = pd.read_csv(filename, sep=\"\\t\", encoding='utf-8')\n",
        "list_stopwords = data['stopwords']\n",
        "\n",
        "def remove_stopword(text):\n",
        "    pre_text = []\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word not in list_stopwords:\n",
        "            pre_text.append(word)\n",
        "    text2 = ' '.join(pre_text)\n",
        "\n",
        "    return text2\n",
        "\n",
        "def sentence_segment(text):\n",
        "    sents = re.split(\"([.?!])?[\\n]+|[.?!] \", text)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def word_segment(sent):\n",
        "    sent = tokenize(sent)\n",
        "    return sent\n",
        "\n",
        "path_to_corpus = '/content/text'\n",
        "\n",
        "\n",
        "f_w = open('./datatrain.txt', 'w')\n",
        "for i, sub_dir in enumerate(os.listdir(path_to_corpus)):\n",
        "    path_to_subdir = path_to_corpus + '/' + sub_dir\n",
        "    for j, file_name in enumerate(os.listdir(path_to_subdir)):\n",
        "        with open(path_to_subdir + '/' + file_name) as f_r:\n",
        "            contents = f_r.read().strip().split('</doc>')\n",
        "            for content in contents:\n",
        "                if (len(content) < 5):\n",
        "                    continue\n",
        "                content = clean_text(content)\n",
        "                sents = sentence_segment(content)\n",
        "                for sent in sents:\n",
        "                    if(sent != None):\n",
        "                        sent = word_segment(sent)\n",
        "                        sent = remove_stopword(normalize_text(sent))\n",
        "                        if(len(sent.split()) > 1):\n",
        "                            f_w.write(sent + '\\n')\n",
        "            print(f'Done , {i + 1}, :, {j + 1}')\n",
        "\n",
        "f_w.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVPV3uelI_mN",
        "outputId": "a46a2b42-32b7-4ff2-ec84-58d3df625016"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done , 1, :, 1\n",
            "Done , 1, :, 2\n",
            "Done , 1, :, 3\n",
            "Done , 1, :, 4\n",
            "Done , 1, :, 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qE6_ZvFHMd9K"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}